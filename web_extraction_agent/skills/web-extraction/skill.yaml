id: web-extraction-v1
name: web-extraction
version: 1.0.0
author: paras@getbindu.com

# Description
description: |
  Extracts and structures web page content by transforming unstructured HTML
  into organized, standardized data sections. Identifies titles, descriptions,
  content sections, links, contact information, and metadata to produce
  consistent structured output suitable for research, monitoring, and automation workflows.

# Tags and Modes
tags:
  - web
  - extraction
  - structured-data
  - scraping
  - content-analysis
  - research
  - automation
  - metadata
  - parsing

input_modes:
  - text/plain
  - application/json

output_modes:
  - text/plain
  - application/json

# Example Queries
examples:
  - "Extract all information from https://www.example.com"
  - "Get product details from this page"
  - "Extract contact information from this website"
  - "Analyze webpage structure from this URL"
  - "Scrape article content from this link"

# Detailed Capabilities
capabilities_detail:
  primary_capability:
    supported: true
    description: "Structured extraction of publicly accessible web page content"
    features:
      - "Extracts page title and description"
      - "Identifies structured content sections"
      - "Extracts internal and external links"
      - "Identifies contact information when available"
      - "Extracts visible metadata"
      - "Normalizes output into consistent structure"
    limitations: |
      Requires publicly accessible web pages.
      Cannot bypass authentication, paywalls, or CAPTCHA systems.

  secondary_capability:
    supported: true
    description: "Content organization and structural assessment"
    features:
      - "Organizes content into logical sections"
      - "Identifies product features or service highlights"
      - "Detects page type (article, product, documentation, company site)"
      - "Provides structural clarity assessment"
      - "Supports multi-section hierarchical output"

# Requirements
requirements:
  packages:
    - "agno"
    - "exa"
    - "firecrawl"
    - "openrouter"
    - "mem0"
  system:
    - "Internet access for URL retrieval"
  min_memory_mb: 512

# Performance Metrics
performance:
  avg_processing_time_ms: 15000
  max_concurrent_requests: 2
  memory_per_request_mb: 512
  scalability: horizontal

# Tool Restrictions
allowed_tools:
  - Read
  - Extract
  - Analyze
  - Write

# Rich Documentation
documentation:
  overview: |
    This skill extracts structured data from publicly accessible web pages.
    It converts raw HTML into organized sections including titles, descriptions,
    content blocks, links, contact details, and metadata.

    Designed for research, automation, competitive analysis, documentation
    building, and knowledge aggregation workflows.

    Built on the Agno framework with OpenRouter for content reasoning
    and optional Mem0 for session memory.

  use_cases:
    when_to_use:
      - "Extracting structured information from company websites"
      - "Scraping product details from e-commerce pages"
      - "Aggregating article content for research"
      - "Building knowledge bases from web sources"
      - "Collecting metadata and structured content"
      - "Website structure analysis"

    when_not_to_use:
      - "Bypassing paywalls or subscription systems"
      - "Extracting private or authenticated content"
      - "Real-time high-frequency scraping"
      - "Mass crawling without rate limiting"
      - "CAPTCHA-protected websites"
      - "Visual or image-based content analysis"

  input_structure: |
    Accepts URLs in plain text or structured JSON format.

    Example (Plain Text):
    "Extract information from https://www.example.com"

    Example (JSON):
    {
      "url": "https://www.example.com",
      "focus": "full | metadata | links | contact | content",
      "content_depth": "basic | standard | detailed",
      "include_links": true
    }

    Constraints:
    - Must contain a valid HTTP or HTTPS URL
    - Publicly accessible page only
    - One URL per request
    - Maximum recommended response size enforced by system limits
    - No authentication-based content extraction

  output_format: |
    Returns structured extraction results with standardized sections.

    Output Structure:
    - PAGE SUMMARY: Brief summary (< 30 words)
    - PAGE METADATA:
      - URL
      - TITLE
      - DESCRIPTION
      - PAGE TYPE (article | product | documentation | company | other)
    - CONTENT SECTIONS: For each section:
      - SECTION TITLE
      - SECTION CONTENT (condensed text)
    - FEATURES / KEY HIGHLIGHTS: Important bullet points if detected
    - LINKS:
      - INTERNAL LINKS (list)
      - EXTERNAL LINKS (list)
    - CONTACT INFORMATION:
      - EMAIL
      - PHONE
      - ADDRESS
    - STRUCTURE ASSESSMENT:
      - CONTENT ORGANIZATION (High/Medium/Low)
      - INFORMATION CLARITY (High/Medium/Low)
      - COMPLETENESS (High/Medium/Low)
    - EXTRACTION METADATA:
      - EXTRACTION METHOD USED
      - PROCESSING TIME (ms)
      - SUCCESS (true/false)

  error_handling:
    - "Invalid URL: Returns error requesting valid HTTP/HTTPS link"
    - "Access denied: Returns access_denied message"
    - "Timeout: Retries with exponential backoff, max 3 attempts"
    - "Content parsing failure: Returns partial extraction with explanation"
    - "Unsupported content type: Returns unsupported_content_type error"
    - "Rate limit reached: Returns rate_limit_exceeded message"

  examples:
    - title: "Company Website Extraction"
      input:
        url: "https://www.company.com"
      output:
        page_summary: "Company website presenting services and contact information."
        sections_detected: 5
        processing_time_ms: 14200

    - title: "E-commerce Product Extraction"
      input:
        url: "https://store.example.com/product/123"
        focus: "full"
      output:
        page_summary: "Product page detailing specifications and pricing."
        sections_detected: 6
        processing_time_ms: 15800

  best_practices:
    for_developers:
      - "Respect robots.txt and website terms of service"
      - "Implement rate limiting and caching"
      - "Validate URLs before extraction"
      - "Handle JavaScript-rendered content gracefully"
      - "Provide attribution when required"

    for_orchestrators:
      - "Route web URL extraction requests to this skill"
      - "Validate URLs before forwarding"
      - "Limit concurrency to 2 requests"
      - "Apply timeout policy (~60 seconds)"
      - "Cache extraction results for repeated URLs"

  installation: |
    Installation instructions for the web-extraction agent.

    Required packages:
    uv sync

    Environment variables:
    - OPENROUTER_API_KEY: Required for reasoning
    - EXA_API_KEY: Required for extraction
    - FIRECRAWL_API_KEY: Optional for advanced crawling
    - MEM0_API_KEY: Optional for memory

    Quick start:
    1. Clone repository
    2. Configure environment variables
    3. Run: uv venv --python 3.12 && source .venv/bin/activate
    4. Run: uv sync
    5. Start agent: python -m web_extraction_agent.main

  versioning:
    - version: "1.0.0"
      date: "2026-01-12"
      changes: "Initial release with structured content extraction and normalization"

# Assessment fields for skill negotiation
assessment:
  keywords:
    - "web extraction"
    - "scrape website"
    - "extract content"
    - "parse html"
    - "structured data"
    - "website analysis"
    - "metadata extraction"
    - "contact extraction"
    - "page structure"

  specializations:
    - domain: "web content extraction"
      confidence_boost: 0.4
    - domain: "structured data processing"
      confidence_boost: 0.3
    - domain: "research automation"
      confidence_boost: 0.3

  anti_patterns:
    - "real-time chat"
    - "video processing"
    - "image generation"
    - "audio transcription"
    - "code writing"
    - "mathematical solving"

  complexity_indicators:
    simple:
      - "extract page title"
      - "get metadata"
    medium:
      - "structured content extraction"
      - "link classification"
    complex:
      - "multi-page crawling"
      - "dynamic JavaScript content"
      - "e-commerce structure analysis"
